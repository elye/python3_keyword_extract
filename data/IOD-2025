[MUSIC PLAYING] [APPLAUSE]
JOSH WOODWARD: Hello, developers. Hello, hello. It's great to be back up here with you.
And earlier today, you heard all about our updates to Gemini 2.5 Pro and Flash.
And we talked about how we're turning research into reality, how we brought 2.5 Pro to our coding agent,
Jules, which I understand is having a lot of demand and our TPUs are hot right now.
That idea of bringing innovation to life is something we explore every day in Google Labs,
and that's our dedicated space to try stuff out. So today, we wanted to try something different with the show here and bring that same spirit
so you can build with Gemini across Android and the web and beyond. So here's how this is going to work for this new format.
We're going to try to do as many demos as we can in 59 minutes, OK? [APPLAUSE]
All right. So there's only two things you have to do as an audience. If it works-- if the demo works, what are you going to do?
Applaud. [APPLAUSE] Yeah, hands up. That works, too. If it doesn't work for some reason,
what are you going to do? Applaud. [APPLAUSE] OK. All right. I'm going to jump right in.
Building with Gemini
I'll do the first one. And over in Labs, we love to build with Gemini 2.5 Flash.
Does anyone use the Flash model out there? All right, a few people. Good.
It's great. It's insanely fast. It has a great price. We use it for a lot of our prototyping.
And one of the things we want to show you today is how we're blending code and design
so you can go from a prompt to an interface to code in just a minute or so.
So it's a new Labs experimental project called Stitch, and we'll bring it up on the screen here.
And first thing is first. Did anyone come to I/O for the first time?
Maybe your first time in California, too? All right, great. We're going to build an app for you.
So here it is. This is Stitch. It starts with design first. So you can go into it and just paste in a prompt,
like make me an app for discovering California-- the activities, the getaways. And basically, you can just click Generate Design,
and it's going to go out and start making a design for you. And so I've done this just before.
It takes about a minute or so, but just to save time and get more demos in an hour here, these are some of the screens it comes back with.
Now, these are not just static screenshots. These are actual designs. And what's cool is you can either iterate here on the left
and iterate-- and create new designs, or you can go in here and do things like make it dark mode, lime green, max radius,
and it'll apply it and start working in the background. Now, what's happening here is we're
using both 2.5 Pro or 2.5 Flash to start letting you remix this and change it around.
So this will work for about a minute or so, and I'm going to show you the one that comes back. This is it.
It is dark mode, lime green, massive corner radius. [APPLAUSE]
And what's really cool is if you go into these, you can see that you can go in and grab the markup right here.
It's all here. You can copy this out to whatever IDE you want.
Again, these aren't screenshots. They're actual designs. You can take it right into Figma and edit it further.
So this is a new product. It's experimental. We want you to try it out and give us feedback. Labs.Google/Stitch.
And that's the first demo. All right. [APPLAUSE]
So with that, I want to pass it off to Logan, who's going to tell you all kinds of great things you can build with the Gemini API.
Logan. [MUSIC PLAYING] [APPLAUSE]
LOGAN KILPATRICK: Thanks, Josh. I'm excited to be at I/O today to show you some of the cool things we just shipped this morning.
We'll start in AI Studio, where the goal is to help you answer the question, can I build this with Gemini,
and then get you started building with our latest models. OK, so let's open up Google AI Studio.
I've been wanting to build a bunch of AI voice agents, but I've been a little busy. No better time than now to try prototyping.
Earlier, you saw how Project Astra can make AI feel natural, and a lot of those capabilities are now available today
in the Live API. So let's select the new 2.5 Flash native audio model.
We've also added more controls like proactive audio.
The model is now better at ignoring stray sounds or voices, perfect for the I/O stage.
It also natively supports 24 languages. We've added new controls for managing your session contexts,
which you can see on the right-hand side here. But in order to build an agent like Astra,
the model needs to be able to use tools. So we've added improvements to function calling and search
counting. We're also introducing a new tool we're calling URL Context.
URL Context enables Gemini models to access and pull context from web pages with just a link.
This is exciting because now you can ground model responses on both specific and up-to-date information,
with support for up to 20 links at a time. All right, let's pass the link to our Developer docs
for function calling and have Gemini give us the TLDR.
[COMPUTER TONE] SPEAKER 1: Google's function calling lets you connect Gemini models to external tools and APIs,
allowing them to understand when to use specific functions and provide the necessary parameters to execute
real-world actions. Acting as a bridge-- LOGAN KILPATRICK: Nice. I couldn't have said it better myself.
[APPLAUSE] This is just a small glimpse into what we've landed today
in the Live API, and we're super excited to see what you build. All right, let's shift gears now and show you
another way we're making it easier for you to build with Gemini.
As you heard from Tulsee earlier today, Gemini 2.5 Pro is awesome at coding. We've seen the ecosystem loving 2.5 Pro,
so it's only logical that we bring it into AI Studio's native Code Editor.
Let's kick off an example of what this new experience is like using one of the preset prompts.
This new experience is tightly optimized with our SDK, so you can quickly generate web applications that use the Gemini
API, like this AI-powered adventure game that uses both Gemini and Imagen.
As you see here, the model is reasoning about the request and composing a spec for the app.
It's then going to generate the code and will actually self-correct if there's any errors. This is going to take a few minutes,
so we'll jump over to another tab I ran to show the final output.
That looks awesome. [APPLAUSE] This is a great way to iterate quickly with the Gemini API,
and you can easily make code changes right here. The whole experience is built to be multi-turn and iterative
so you can keep refining your idea with prompts over on the left-hand side.
All right, we've got time for one more update that I'm excited to talk about, which is around MCP.
Today, we're rolling out an update to the Google GenAI SDK to natively support MCP definitions.
[APPLAUSE]
Now, it's even easier for you to build agentic apps with a fast-growing community of open-source tools.
To showcase this, we've added a new app using Google Maps and MCP.
Let's welcome Paige to the stage to start building on it. [APPLAUSE]
PAIGE BAILEY: That was perfect. [MUSIC PLAYING, APPLAUSE]
You did great. Excellent. Thank you so much, Logan. I took inspiration from this Maps app
that Logan just showed you, and I'm going to remix and compose it to build something entirely new.
I'm going to add a talking head that will accompany me on stage, my very own keynote companion,
but I'm just going to call her KC. I want our keynote companion to listen to our keynote,
to respond, and then to dynamically update its UI based on what it hears. And I've got a few finishing touches to add,
but I think we can wrap it up here together on stage. So in IO tradition, the first thing
that I'm going to need my keynote companion to do-- KC: Hi, I'm KC, Paige's keynote companion.
I'm here to help her presentation go smoothly. PAIGE BAILEY: --is to count the number of times we say, "AI."
KC: Got it, Paige. I'm on it. PAIGE BAILEY: Excellent. So to help us keep track, we've developed a function
called Increment Utterance Count, or Increment Word Count, that counts and updates the UI.
This is how you would build a conversational agent to listen to specific audio inputs, and then to execute a function to perform an action.
But counting AI instances was so last year.
So let's update this to trigger every time the presenter says something Gemini-related.
So I'm going to make those updates. I'm going to refresh.
And we're going to try again. KC: Hi, Paige. I'm KC, your keynote companion.
PAIGE BAILEY: All right. So the Gemini Live API supports a sliding context window.
That means that your users can interact with your apps for long-running sessions, or in our case today,
they can run quietly in the background. And awesome, just look at that. We were able to make this app dynamically react
to what it's hearing, which means that it's a dynamic web page, if you will.
Get it? Get it, KC? Ah, excellent. [LAUGHTER] KC: Should we take a quick look at a fun fact about the Gemini
API? PAIGE BAILEY: [CHUCKLES] Hold on there. We're about to get to that. So this is just a fun example, but imagine
discussing gift ideas in a shopping app, or finding a great restaurant that delivers to you with just your voice.
So now, let's see how that Maps integration works.
Hey, KC, since there are people coming from all over the world, could you show us Shoreline Amphitheater on a map?
KC: All right, I've put Shoreline amphitheater on the map for everyone. Is there anything else I can help with?
PAIGE BAILEY: Yeah, could you show us some good coffee houses nearby, with really great Wi-Fi? [LAUGHTER]
KC: OK, I've searched for coffeehouses with Wi-Fi near Shoreline Amphitheater. PAIGE BAILEY: Excellent.
Could you randomly select one that you think looks really good, and give us directions there?
KC: While I can't express preferences, I can provide-- PAIGE BAILEY: Yeah, so just pick up--
how about Boba Bliss? Give us directions to Boba Bliss from Shoreline Amphitheater.
KC: OK, I'm providing directions to Boba Bliss from Shoreline Amphitheater. Is there anything else I can help with at the moment?
PAIGE BAILEY: No, that was awesome. Thank you, KC. Excellent. [APPLAUSE]
Amazing. So let's check out some more feature updates.
While synchronous function calls are used for quick operations, high-latency tasks, for things like calling an MCP server,
usually require background processing for a seamless user experience. So by default, when calling a function, the audio is blocked.
But today, we're excited to enable asynchronous execution for seamless dialogue
within a conversation. [APPLAUSE] Yeah, over here, I have a function called 'getFunFact',
that uses Gemini 2.5 Pro. Based on what it said, we're going to use search grounding to display a fun fact.
And up here in the system instructions, I've also added the 'getFunFact' function whenever the presenter
mentions one of our Google AI products. But I'm going to add a new call, called behavior--
I'm going to stop this so it doesn't automatically update-- but say behavior.NON_BLOCKING.
And you can see the automatic code execution kick into play in AI Studio applets.
Finally, we heard your feedback. With this latest release, we've improved structured outputs
with function calling. So our model is going to conform with this very specific JSON
return format to make sure that everything displays beautifully in the UI. All right, moment of truth
KC: Hello. I'm KC, Paige's keynote companion. PAIGE BAILEY: Yes, I understand. I understand. But I am biased, but I think that Google AI
Studio is the best place to start building with the Gemini API.
Woo! Excellent. Heck, yeah. KC: Yes, Paige. Google AI Studio is pretty cool.
Did you know that it lets you experiment with different models and parameters, without writing any code?
PAIGE BAILEY: Yeah, absolutely. Look at that. Gemini and KC, you are a natural.
My keynote companion is feature-complete. I think it's time to deploy these changes and to see it in action.
So once your app has been created in AI Studio, it's incredibly easy to share with your friends,
but also to deploy it via Cloud Run. So right here, from within the AI Studio,
I'm going to kick off a Cloud Run deploy. I'm going to select one of my projects.
It's going to verify. And with one button click, we're already deploying this app in a way that many people
throughout the audience and throughout the world would be able to see. Once this app has been deployed, it's
also capable of being run and viewed with your favorite IDE.
So we can see it. We can refresh. And you can see the keynote companion automatically added
in Cloud Run right within my VS Code instance.
And just like that, our multimodal app is live.
Pun intended. [APPLAUSE] Awesome.
We're making it easy for you to build agents with Gemini, combining multimodal reasoning with a vast and a growing
number of tools. This power extends beyond the web, right into the palm of your hand.
So now let me welcome Diana and Florina to the stage to talk about new tools and AI advancements
for the Android ecosystem. Woo! [MUSIC PLAYING, APPLAUSE]
Android
DIANA WONG: You've just heard about the incredible possibilities that AI brings.
Now, Florina and I want to talk to you about how you can build excellent apps powered
by AI on Android, and then how our APIs and tools make it possible to be more productive using AI.
FLORINA MUNTENESCU: Exactly, Diana. An excellent app is delightful, performant,
and works across devices. And with AI, you can unlock entirely new, unique experiences
that bring value to your users. Let's show you how that comes to life with a new app we built.
A few years ago, we had a website that let you build yourself as a cute Android bot,
selecting things like clothing and accessories. But then we started thinking, how would we build
this today using AI as an app? The answer was, of course, through selfies and image
generation. So we came up with Androidify. Let's take a photo of Diana and Androidify her.
DIANA WONG: Let me grab my favorite toy as an accessory. It's my daughter's. FLORINA MUNTENESCU: Ooh.
So while Androidify is Androidifying, let's see what's happening under the hood.
DIANA WONG: So the core of the app relies on two key AI-powered steps, getting a description
of the person in the photo, and then creating an Android robot out of the description. Without AI, creating this experience
would be nearly impossible. And to implement this, we used AI models running in the Cloud via Firebase.
FLORINA MUNTENESCU: To get the description of the image, we took advantage of the fact that Gemini models are
multimodal, so they can use text and also images, videos, and audio as input.
So all we had to do is call Generate Content with a text prompt and the image the user provided.
So the image of Diana. Then to generate the Android robot based on the image description, we used the Imagen 3 model.
We call Generate Image. And that's it. How easy was this? OK, are you ready to see this?
DIANA WONG: I think so. FLORINA MUNTENESCU: Ta-da. DIANA WONG: Oh, that's so cute. [APPLAUSE]
Androidy with Android. You did all of this with what? AI and five lines of code?
That's so cool. FLORINA MUNTENESCU: The Androidify sample app is already available on GitHub for you to check it out.
[APPLAUSE] DIANA WONG: As Florina showed, Cloud-based models
are powerful and ideal for Androidify. But what if you need to process prompts locally, directly on the device without sending data
to a server? That's where on-device AI shines. Gen AI APIs powered by Gemini Nano,
our multimodal on-device model, offer APIs for common tasks like summarize, rewrite, and image
description. FLORINA MUNTENESCU: So we said earlier that excellent apps are delightful, performant, and work
across devices. Let's talk about delightful apps first.
For those of you who caught the Android show last week-- and if you didn't, definitely check it out.
You saw our biggest UI redesign in years, packed with delightful new features and improvements.
We're helping you bring the same level of delight and playfulness to your own apps, with an update to the Material 3 design system,
called Material 3 Expressive. We've already used it in our Androidify app.
For example, take the Camera button. You could use a circle, or you could use the cookie shape
from the new shape library. DIANA WONG: And who doesn't love cookies? FLORINA MUNTENESCU: The button group shape
morphs in the photo prompt button is so nice and smooth, and it's part of the Expressive APIs.
These small details are what separates a good app from a delightful app.
Try out these APIs yourself using Compose Material alpha. DIANA WONG: Beyond material design,
what else makes an app delightful? Helping users with more useful, relevant information.
In Android 16, we added a new feature, live updates. They allow you to show time-sensitive updates
for navigation, deliveries, or rideshares by using the new ProgressStyle template rolling out
to devices over the next year. FLORINA MUNTENESCU: Now, let's shift gears to another critical aspect of an excellent app, performance.
Make sure you enable R8 and Baseline profiles. Both of these have been available for a while,
and the performance results are impressive. And they translate into better Play Store ratings.
With R8 and Baseline profiles, Reddit's app improved so much they got a full star rating
increase within two months. DIANA WONG: Next, an excellent app
looks good across all the devices that users have, and we're making it easier for any app to move across devices, out of the box,
from foldables and tablets to Chromebooks. In Android 16, we're making API changes
to no longer react to orientation, resizability, and aspect-ratio restrictions, giving users
more responsive UIs by default. FLORINA MUNTENESCU: And we're putting your app in more places.
For example, we've been collaborating with Samsung, building on the foundation of Samsung DEX
to bring enhanced desktop windowing capabilities in Android 16 for more powerful productivity
workflows. DIANA WONG: To make your app beautiful across devices, we're continuing to make it as easy as possible
to build adaptively with new features in our Compose Adaptive Layouts library, like pane expansion.
FLORINA MUNTENESCU: Optimizing your app to be adaptive has a real impact on business metrics.
We've seen that when users engage with an app across multiple devices, in app categories like music, entertainment, and productivity,
there's a 2 to 3 times increase in engagement. And apps like Canva, who invested in large screens,
found that cross-screen users are twice as likely to use Canva every single week.
DIANA WONG: But it's not just foldables and tablets. We're bringing Android apps to more devices, automatically.
So here's the great part. If you're building adaptively for Android, you're already building for two more form factors, cars and XR.
With cars, whether users are waiting at a charging station or in line at school pickup, they
can stay entertained with popular streaming and gaming apps like Farm Heroes, Saga, and more.
You can adapt your existing large-screen app to be used in parked cars by simply opting into distribution via Play Console
and making minor optimizations. And if you're building adaptively,
you're building for Android XR, the extended reality platform built together with Samsung.
It powers glasses like the ones Nishita demoed earlier. We'll share more details on how you can develop for these later
this year. It also powers headsets, like Project Moohan from Samsung,
which you can start building for right now, knowing it'll be in the hands of consumers later this year.
Soon after, our partners at XREAL will release a Developer edition of the next Android XR device,
codenamed Project Aura. It's a portable device that gives users access to their favorite Android apps,
including those that have been built for XR. All the apps you're developing will scale directly
to these two. FLORINA MUNTENESCU: With these upcoming devices, we're launching a Developer Preview 2 for Android XR SDK,
with new material XR components, updated emulator support in Android Studio, and spatial video support
for your Play Store listings. DIANA WONG: OK, that's a lot of form factors, and we're enabling more for users out of the box.
I love seeing when my apps look great across all of my Android devices. Once an app is adaptive, we unlock access
to more than 500 million devices it can run on. We're already seeing incredible experiences with partners,
whether it's Peacock, who put in the work to create a strong, adaptive experience for their large screen app, and as a result,
get a really nice XR app. FLORINA MUNTENESCU: Or like Calm, who easily extended their Compose app to create sensory, mindful experiences only possible
with XR. DIANA WONG: Now, building these kinds of excellent experiences across devices means you need powerful tools.
And that brings us to our next topic, boosting your productivity. FLORINA MUNTENESCU: It's no surprise
that we use Jetpack Compose for Android devices. It has the features, performance, libraries,
and tools that we need to build an excellent app. 60% of the top one thousand apps take
advantage of the development speed Compose offers. The latest stable release brings the features
you've requested, like autofill, text auto-size, and visibility tracking.
We're focused on making Compose performance better and better. In the latest release, we see barely
any janky frames on an older generation Pixel device. And we heard your feedback.
You want to use Compose throughout your UI, so we're releasing CameraX and Media3 Compose libraries.
DIANA WONG: We know building navigation. Oh, yeah. Clap if you want. [LAUGHS] [APPLAUSE]
We know building navigation for apps across different devices
and screen sizes can be complicated. So we rebuilt the Jetpack Compose navigation library
from the ground up. Our goal was to make it simpler, more intuitive, and incredibly
powerful for managing screens in a stack, retaining state, enabling seamless animations and adaptive
layouts. FLORINA MUNTENESCU: When we think about productivity, it's not just about writing code faster.
It's about streamlining the entire development life cycle, from refactoring to testing and even fixing crashes.
That's where Gemini and Android Studio truly shines, by taking on those tedious tasks.
DIANA WONG: So let's head back to the demo desk to show you a few features that will change the way that you work.
FLORINA MUNTENESCU: So we all know the benefits of writing end-to-end tests. You get to test large parts of your app at the same time.
But we also know that these are the ones we avoid doing the most because they tend to be hard to implement.
DIANA WONG: You can now use natural language to perform actions and make assertions with Gemini in Android Studio.
So let's bring together some of the Androidified features we've shown so far and test them.
The files here are journeys, like user journeys. Let's run one of them.
FLORINA MUNTENESCU: So if you read the first two actions, they're just clicking on certain buttons
with different text on them. So click on Let's Go. Tap on Choose Photo. The journey is waiting until the actions are done before moving
to the next action. So you don't have to synchronize these tests anymore. Then the third one is a bit more interesting.
Select a photo of a woman with a pink dress. This is where natural language is a lot more powerful.
So this could be the third photo, like here. Or it could be the first one. So it would be hard to find with a regular command.
Also, the UI you just saw is the platform's photo picker, and it can change from version to version of Android.
The last action was just verifying that an Android figurine with an umbrella is displayed.
And that's it. The test passed. [APPLAUSE]
DIANA WONG: Well, that was so easy even a PM like me could do it. FLORINA MUNTENESCU: Oh, so you're volunteering.
DIANA WONG: Let's change topics. How about updating dependencies to the latest versions? I know that's another task that developers "love" doing.
FLORINA MUNTENESCU: So it's definitely something I avoid doing because it's a tedious task, although I
know. We get the benefits of features and bug fixes that come with the latest libraries.
Now, Gemini can help with this, too. So let's demo a new AI agent, coming soon in Android Studio,
to help with version updates. I have a project loaded here that-- well, I haven't really touched it in a year,
so this was actually before, even, Kotlin 2.0. There's a new option in the refactor menu
called Update Dependencies. So what the bot has just done here is analyzed the different modules in the app
and checked what library updates we can apply. Next, the agent is trying to build a project
and then use Gemini to figure out how to fix the problems. Then it's going to iterate until the build is successful.
Let's give it a second and then see how it behaves. OK, so it found an issue.
The build succeeded. It's generating the upgrade summary. Let's see if we get to see the changes.
OK, I'm going to click Show Changes. I'm going to minimize this just to have a bit more space. So the libraries have been updated to the latest version.
In the plugins, we're using the new Compose Compiler-- Compose plugin compiler.
The Compiler SDK is now 36. And then in the main activity, we not only get the change,
but we also get an explanation of the change. So we can see that the platform class was replaced
with an AndroidX class. Same here. So the build is successful.
So we're done. DIANA WONG: Nice. [APPLAUSE]
Now that you've seen some of the powers of Gemini and Android
Studio in action, you might be wondering how you can get the same benefits in a corporate and enterprise environment,
not just your personal projects. So today, once you subscribe to Gemini Code Assist,
you'll have access to Gemini and Android Studio for businesses. It's specifically designed to meet the privacy, security,
and management needs of businesses, with the same Gemini and Android Studio that you're used to. FLORINA MUNTENESCU: As you've seen today,
across all of Android, we're making it easier for you to build the best experiences, creating
apps that are delightful, performant, and adaptive across devices. And we're helping you be more productive with Compose
and Android Studio. DIANA WONG: So check out the Androidify app to see how this all comes together.
Use the latest versions of our APIs and tools. And go build the next generation of Android apps.
Now, let's hear from Una and Addy about how you can go build the next generation of web apps.
[MUSIC PLAYING] [APPLAUSE]
Web
UNA KRAVETS: One of the best things about the web is that its reach is virtually unlimited.
With a single website, you can bring your ideas to almost any user on the planet.
The challenge is creating applications that work well across the near-infinite combination
of users, devices, and browsers. Today, I want to introduce you to a slew
of powerful new features in Chrome that will help you do just that.
With these features, you can build better UI, debug your sites more easily with DevTools,
and create AI features more quickly and cost-effectively with Gemini Nano in Chrome, all of these
with the goal of creating a more powerful web made easier. Building engaging user interfaces is so critical.
This is the cornerstone of what sets you and your app apart in a very crowded digital space.
So let's begin there. We've been hard at work fixing core issues and expanding capabilities by leveraging
HTML and CSS, the web's native building blocks. It should not only be possible, but simple
to create beautiful, accessible, declarative, cross-browser UI.
What better way to make this more tangible than by creating a website, using everything
that we're announcing today? We'll show you some new capabilities that make it easier to build common but surprisingly complex
UI elements, like carousels and hover cards. And we're going to build these right here
on stage in a fraction of the time. To do that, my colleague Addy is going to help me out.
[MUSIC PLAYING, APPLAUSE]
ADDY OSMANI: Thanks, Una. So here we have this work-in-progress virtual theater
site built in React. Watch as we transform it into a rich, delightful experience,
starting with turning these posters into a carousel. Now, there's a lot to juggle--
transitions, state management, DOM manipulation, performance. And after all that effort, it's still buggy in Edge cases.
Now, with Chrome 135, we've combined a few new powerful CSS
primitives to make building carousels and other types of off-screen UI dramatically easier.
Watch how you can build this in just minutes with only a few lines of code.
So I'm going to head over to my editor, and I'm going to show you this carousel class. So here, I'm positioning the items,
setting overflow and requiring snapping at the center for each item.
So we're just going to save this. We're using our carousel class on our poster list. And we're making some progress here.
It's looking like a carousel. Great stuff. Now, let's add some navigation affordances
for the off-screen items. So I've got a class here-- I've got a controls class.
Let me just add that, as well, to that poster list and show you that. So we've got some buttons using this controls class.
And I want to show you the code for this. So here, you can see that those buttons were created using
scroll button pseudo-elements. And this is what the code looks like. We've got a scroll button right and a scroll button left.
Now, we've styled these with a content property. Here's the content property down here.
And we've given them accessible labels. UNA KRAVETS: Can we navigate through this carousel
a little bit faster? I've seen a lot of them have those little navigation dots on the bottom. ADDY OSMANI: Yes, we can do that.
So let me show you our indicators. That's what the new scroll marker pseudo is for.
So this is another pseudo-element which can be styled similarly to scroll buttons.
So here's our scroll marker down here. And we've also got another new pseudo-class called target
current that manages the active marker classes. And that's target current right over there.
So let me just add in our indicators class, and, bam, we now have scroll indicators.
[APPLAUSE]
UNA KRAVETS: One early adopter of this new CSS-based method of building carousels is Pinterest.
Before, their development team spent so much time maintaining their custom-built JavaScript carousels.
But earlier this year, Pinterest made the switch to using new CSS APIs for carousels, cutting down around 2,000 lines of JavaScript
into just 200 lines of more performant, browser-native CSS. [APPLAUSE]
That whopping 90% reduction in code
improved overall performance, and it also noticeably improved product pin load times by 15%, which
is a huge quality-of-life improvement for their users, like me. Addy, how's our carousel going?
ADDY OSMANI: I thought it could use a little more finesse, so I added in some hero images.
Here they are. They look a little bit pretty. And I also put it on a virtual stage.
So check this out. UNA KRAVETS: Ooh, fancy.
ADDY OSMANI: Isn't that nice? That's using scroll-driven animations. [APPLAUSE] Now, I would love to put in a feature
where you could easily hover and get a sense of what the view is like from there. So I'm going to add in the seat sections and seat details now.
UNA KRAVETS: I love that idea. Rich tooltips and preview cards are super common in web interfaces, but building and maintaining
them, managing state accessibility hooks and events is still extremely challenging.
So I can't wait for you to try the new experimental interest invoker API, which, when used with the existing
anchor positioning and popover APIs, helps you build out accessible, complex, layered UI elements
without JavaScript in a fraction of the time. ADDY OSMANI: All right. So in this theater layout, we now have seat sections.
Let's add in a seat preview for each of these sections so you can get a sense of what the stage looks like from them.
So I'm just going to go ahead and click this eye icon. And, ooh, check that out.
So right, now this is a popover-- all right. This is a popover-- [APPLAUSE]
This is a popover which is triggered on click. So let's have it show up when hovering or focusing instead.
So to make this change, all we need to do is change our popover target to interest target.
So we go back to our editor. I've got our eye button here. And I'm just going to change this popovertarget.
I'm going to type in interesttarget instead and hit Save.
Let's do that again. Let's hover over the eye. Nice. [APPLAUSE]
So here, the browser is handling the state management,
event listeners, ARIA labeling and a lot more for you, making this complex interaction a breeze.
UNA KRAVETS: Building something like this was so hard to get right, and I can't believe how easy it is now.
This is the power of modern CSS, turning complex UI challenges
into straightforward, declarative code without frameworks, multiple libraries, and thousands
of lines of JavaScript. But I know the big question that you're all asking is, will this work for my users?
That's why we launched Baseline, to show you feature availability across all major browsers.
But we know for Baseline to be truly useful, it has to be available in the tools that you use every day, like IDEs, linters,
and analytics tools. ADDY OSMANI: So here I am, back in VS Code. Look what happens when I hover over some of these recent CSS
additions. Right in the tooltip, you can see its baseline status
and browser availability. No more switching between browser tabs to check on your compatibility.
It's really nice. [APPLAUSE]
UNA KRAVETS: And if these aren't your tools of choice, don't worry. ESLint can now be configured to warn you
about anything that doesn't match your targeted baseline version for HTML and CSS files.
This integration is also coming soon to VS Code-based IDEs and JetBrains WebStorm.
Shall we get back to building our website, Addy? ADDY OSMANI: Perfect timing. So I've made quite some progress.
Come on. Take a look. UNA KRAVETS: Is that Find my Seat button new? It's not centered.
Can we fix it? ADDY OSMANI: Yeah, we can. So sure, this is actually a perfect opportunity to showcase some of our new AI features in Chrome DevTools.
So let's take a closer look. I'm going to open up DevTools, and I'm just going to select the element that we're having some issues with,
that Find my Seat. Now, if I right-click, if I go to ask AI-- check this out.
This is something new. This is AI assistance that is baked in to the panel.
Now, I'm just going to, actually, switch over, right to here to show you this.
One moment.
Let's hope that the demo gods are with me.
Alrighty. So we're going to go back to that Find my Seat.
I'm going to select that one button there.
Amazing. Now, we're going to turn on AI assistance just make sure that is all turned on.
Awesome stuff. So what I'm going to do here is--
[APPLAUSE] Oh, no, no. This is-- UNA KRAVETS: We're doing it live. [APPLAUSE]
ADDY OSMANI: All right, so this is AI assistance that's baked into the panel. What I want to do now is just use natural language.
And I want to ask Gemini a question. What I'm going to say is-- I set margin to 50%, but it's still misaligned.
How do I fix it?
So notice that I'm just using natural language. No complex queries.
I'm just asking in plain language. And it seems to have come back with a solution in mind.
So let's see what it does when it's applied CSS rules for the margin.
I think that what I need to do here is actually apply a transform fix.
So let's see what it says.
Perfect. So it's centered it for us. [APPLAUSE]
And now, normally, I'd copy this fix.
I'd switch back to my editor. I'd find the right file, locate the right spot in the code, you know, the usual drill.
But with Chrome 137, AI assistance can actually create and apply a fix directly from DevTools.
So I've connected DevTools to my local Workspace. So if I expand unsafe changes here
and I hit Apply to Workspace, let's see what happens. So it's doing some magic behind the scenes.
OK. Let's see if the demo gods stay with us here.
All right. [APPLAUSE] UNA KRAVETS: This usually works.
ADDY OSMANI: It should be applied directly to my source code. So ideally, there's no context switching, no copy-paste errors.
We just get immediate results. UNA KRAVETS: Now, Addy, speaking of fixes, why don't we show everybody the completely redesigned performance panel.
ADDY OSMANI: Yes. Let me pull right up. So here, I've collected a quick performance trace.
We're actually going to switch over to the other one. So here, I've collected a quick performance trace. And in the Performance Insights sidebar,
I can see a layout shift culprit. So let's figure out why this comes up. So we've got our layout shift culprit here.
And we've got our Ask AI button. So you see this. This is a game changer. I'm just going to select this layout shift insight.
And I'm going to ask Gemini-- OK, fine. I need to actually switch over to here.
So let me go back to Ask AI, turn this on.
Perfect stuff. And what I'm going to do is I'm going to ask Gemini,
how can I prevent layout shifts on this page? So we're just going to go ahead and have Gemini do its stuff,
and it's come back with a response. So with the current web font, there's actually a lot of layout shift.
And this is already useful, as it gives me a clear direction of what to do next. This is what I love about these new Chrome DevTools features.
They don't just highlight problems. They help you understand and fix them without leaving your workflow.
[APPLAUSE]
UNA KRAVETS: AI in DevTools is such a great use case, and Gemini is not just helping you debug,
but it also helps you to build. Last year, we announced Gemini Nano in Chrome
and invited you to help shape the future of AI on the web. Since then, nearly 17,000 of you signed up for the early preview
program, and we learned so much from you. Starting today, we're rolling out
seven AI APIs across various stages of availability. Backed by our best on-device models, from Gemini Nano
to Google Translate, these APIs in Chrome have been fine-tuned for the web.
We're also working with other browsers to offer these same APIs, backed by their own AI models,
so that you can use them everywhere. With Gemini Nano and our built-in APIs,
the data never leaves the device. That's huge for schools, governments, and enterprises
with strict compliance and data privacy rules. And it also means that you can affordably scale AI features
to a massive audience. So many of you-- yeah, that's exciting.
[APPLAUSE] So many of you out there are already
using the power of these APIs for your applications, like Deloitte. They're experimenting with an integration
of Chrome's built-in AI APIs right into the Deloitte engineering platform
to improve onboarding and navigation. 30,000 developers at Deloitte can find what they need
at a projected 30% faster, while also giving better feedback to improve the platform.
ADDY OSMANI: So before we wrap up, I want to show you one more thing that's really exciting.
Today we're unlocking new multi-modal capabilities from Gemini Nano. Our multimodal, built-in AI APIs let you create experiences
where users can interact with Gemini using audio and image input. Let me show you how this works.
So going back to our theater example, we can help people to find their seat, like an AI usher.
So for this, we need a function that can extract information from a photo of our ticket
and highlight it in the app. And I want this to work on every device and in every browser.
So we've partnered with Gemini and Firebase to offer a hybrid solution that works everywhere,
on-device or in the Cloud. So over here in our editor, we start by setting up
Firebase and Gemini. And we define our model parameters. I'm just going to click through here
and quickly show you-- we've got a prefer_on_device right over here, set as a configuration.
The AI returned our information here. We're just configuring the image that's being formatted,
and we're finally getting a response. So let's go ahead and give this a try.
Now, normally, I'd be doing this with my phone, but I'm just developing right now.
And so I'm just going to use this webcam. I'm going to snap a photo of this ticket.
Let's try this out. And bam, the built-in multimodal AI instantly--
[APPLAUSE] The built-in multimodal AI instantly located my seat
section in the theater. It's awesome. UNA KRAVETS: You don't have to wait to get
your hands on these new tools. Many of these APIs are broadly available, and you can sign up for the early preview program today
to start experimenting with these new multimodal AI and hybrid solutions.
From building better UI and faster debugging, to creating all-new AI-powered features,
we're constantly working to give you more tools to bring your vision to life.
With your help, together, we can build a more powerful web, made easier.
[APPLAUSE]
And now let's check out how Firebase Studio is making it even easier to spin up a full stack app.
Take it away, David. [MUSIC PLAYING, APPLAUSE]
Firebase Studio
DAVID EAST: Last month, we launched Firebase Studio, a Cloud-based AI Workspace where with a single prompt,
you can create a fully functional app. You can lean on AI assistance throughout or dive into the cod,
thanks to the full power of an underlying customizable VM that is open and extensible, like product manager Erland Van
Reet, who had been kicking around an idea for a flexible platform for sharing things in your community for quite some time,
but was actually able to bring it to life with Firebase Studio.
You can prompt Firebase Studio to generate almost anything, like CRM tools, interview coaches, sales
planners, and games. And today, we're adding more to Firebase Studio
to help you build faster in every part of the development stack, from frontend to back.
Let's start with the frontend. So if you're like me, then there are two stages
you go through when you're handed an incredible looking design. In the first stage, you're excited to work
on a project with such an awesome user interface. And in the second stage, well, you
realize that you actually have to build it. [LAUGHTER] Going from a design tool to a functional user interface,
running in a development environment can take a lot of work. And we wanted to simplify this process.
So now, you can bring your Figma designs right to life in Firebase Studio with some help from Builder I/O.
[APPLAUSE] Now, when you're in Figma, you can install the Builder I/O
plugin, click to export to Firebase Studio, and it translates all the component code and opens up a window with Firebase Studio for you
to kickstart your development process. Let me show you it in action.
So I'm here in Firebase Studio after importing a Figma mock of a furniture store app that
was actually created in Stitch. Now, we have this product grid listing page,
and it's gone from a design to real app code, up and running in a development Workspace.
I can scroll through all of these products, and since it's gone from design to real code,
I can either dive right into the code, or I can ask Gemini in Firebase Studio to give me some advice on where to begin.
And what I love about this Figma export is that it didn't just generate this large monolith of code.
It generated individual, well-isolated components that make up the whole page.
Now, this Figma mock, it only added design for the product grid page, but not a single product detail
page. So let's build one. So I'm going to open up Gemini in Firebase Studio,
and I am going to paste in a prompt, but I will break it all down.
So I'm going to ask it to-- I asked Firebase Studio to build a single-product detail page.
And I wanted to use the existing component system and sample data. Then I'll give it a file name that I want it to use,
and I'll ask it to hook up to the routing system. And lastly, we're going to add a feature
to create an Add to Cart button. Now, before I send in this prompt I can use the model selector.
And I can select to Gemini 2.5 Pro. And now, when I submit this, Gemini in Firebase Studio
gives me a breakdown of all the changes that it wants to make. But it's not going to make them all at once because nobody wants
to review a giant file of code. It's going to break things into multiple steps, making it easier to stay in the loop and review each change.
So it starts by creating the full product detail page. And then after I create this file,
it goes and it updates the routing logic. Then after that, it goes into the product detail card,
and it wraps a link. So it points to-- from the detail card to the individual page.
And then after that, it identifies that it needs to update the Dataflow,
by having the correct Dataflow by passing in the products ID.
And now, this part is really cool. Gemini and Firebase Studio noticed that our product sample
data didn't have a description property, so it wouldn't display any details about the detail page.
So it updated the placeholder data, and it generated descriptions for each product. [APPLAUSE]
And lastly, it generates the Add to Cart button.
And so now, once I update this file and go back into the web page, I can click around,
and I have myself a fully working product detail page. [APPLAUSE]
So all of this was done in just a few minutes with a single import and a single prompt.
So the frontend, it brings the user experience to life. And for apps to truly perform, to handle complex data
and connections, you need a backend. So right now, in Firebase Studio,
you can add your backend either by coding it yourself or with Help from Gemini. But wouldn't it be great if it just added a backend for you.
So normally, when you're prototyping apps in Firebase Studio, we generate an app blueprint that details some of the most important characteristics
of the app, such as features and a style guide. Well, rolling out starting today,
we're adding a backend section to the app blueprints. [APPLAUSE]
Firebase Studio will detect when your app needs a backend
and provision it for you if your prompt includes a database or authentication.
So from this blueprint, Firebase Studio will set up the configuration for the backend services. And then it will also generate code to authenticate users
and save data to a database. And when you're ready to publish, Firebase Studio will provision those backend services
and deploy to Firebase App Hosting. So you can still jump into the coding workspace
and extend your apps with any backend if you prefer a different stack or if your needs change
as your app grows. These features are starting to roll out today, and we're rapidly adding new capabilities to Firebase Studio,
based on your feedback. So try it out now. [APPLAUSE]
Another demo down, and we are into the homestretch. Next, if you want to start tuning your own AI models,
here's Gus to tell you all about what's new with Gemma. [MUSIC PLAYING, APPLAUSE]
Gemma
GUS MARTINS: Hi, everyone. [CHEERING]
I'm very happy to be here with you. Thank you very much. Today has been all about making it easier
for you to build great things with Gemini. But sometimes you really want to fine tune your own model,
like when you want AI to help you understand sensitive data, learn the details of your business, or even run offline.
And that's why we released Gemma, our family of open models. With Gemma open models, we are bringing the magical AI
experiences instantly and privately to our users' hands. A couple of months ago, we launched Gemma 3,
state-of-the-art open models capable of running on a single Cloud or desktop accelerator.
But we kept cooking. And today, I'm thrilled to announce Gemma 3n,
a model that can now run on as little as two gigabytes of RAM. [APPLAUSE]
Cool. Thank you.
Gemma 3n shares the same architecture as Gemini Nano and is engineered for incredible performance.
It's much faster and leaner mobile hardware, compared to Gemma 3. We've also added audio understanding,
make it truly multimodal. We are sharing Gemma 3 in preview today.
It starts on Google AI Studio and with Google AI Edge. And we are also bringing it to open-source tools like Hugging
Face, Ollama and Unsloth and the others all in the coming weeks. The Gemma family is designed to be highly adaptable.
And one domain where this openness and adaptability is showing incredible promise is health care.
And today, I'm very, very excited to introduce MedGemma, our most capable collection
of open models for multi-modal medical text and image understanding.
[APPLAUSE] Yeah, there's a bunch of those.
MedGemma works great across a range of medical, image, and text applications so that developers
like you can adapt the model for their own health apps, for example, when you want to analyze radiology images
or summarize patient information for physicians. OK, I've talked a lot already.
Let me show you something. Let me show you how easy it is to grab a Gemma model
and build something completely unique for you. And so let me go here to a Google Colab.
Nice. And what we are going to do is we will use Unsloth, which is a fantastic
library for fine-tuning LLMs like Gemma, for example, and it's much faster and use less memory.
And it runs great on NVIDIA GPUs that are on Google Colab's free tier.
For this demo, I want to show you how easy it is to fine-tune. My daughter and I, we have a unique emoji language
for texting. It would be great to create a personalized translator for us. For example, when we talk about our dog Luna-- you know what?
Let me show you something. This is Luna. [APPLAUSE] Thank you. AUDIENCE: Aw. GUS MARTINS: [LAUGHS]
So when we talk about our dog Luna,
it should automatically translate to her special emoji. So what I did is I created a custom
data set to fine-tune Gemma to teach it our specific emoji dialect.
And in Colab, we do the same. We load the model, set up the environment-- just the usual.
Colab caches popular models, so it loads really quickly. Training can take some time.
It can take some time. So I've already formatted the data set. I did the training. And now I already have my custom model variant.
And here we go. But the thing is I would like to compare my custom
model against the original one and see if it really learned from my data. And to do that, I'm going to build a UI.
Using the new AI Colab-- new AI-first Colab that we've just launched,
it's an entirely new way to build faster. It's a agentic-first experience that transforms coding into a dynamic conversation that helps
you navigate complex tests. So let me show it for you right here.
Open the feature. Let's put my prompt and execute.
So Colab, now, is generate for me the code. Oh, nice. Done already.
Let's see. And you can see here.
I think everyone is using Colab today. Nice.
It's here. And there it is. So just like that, Colab created this application for me.
But the thing is, we should test this. Any suggestions for prompts?
Anything? AUDIENCE: "Luna loves strawberries." GUS MARTINS: Sure. I can do that.
Luna loves-- sorry. Nervous. [LAUGHTER]
Strawberries, correct? Yes, let's try that. Let's try that. And come on.
Demo. Ooh, worked. Let's zoom in for you. [APPLAUSE]
There you go. You can see the difference between my custom model
and the base one, and that it really knows the language and the details about our emoji dialect.
Isn't that cool? So this is a really fun example that highlights the power
that you have. You take a base model like Gemma. You take accessible tools like Unsloth
and use the new AI-first Colab, and you can quickly build and fine-tune with your own personal data.
And you can also deploy these using Google Cloud with Cloud Run, Vertex. Or you can even use Google AI Edge for local deployment.
But the thing is, my emoji project just scratches the surface, showing how to quickly build on a Gemma
model for your own use case. What is truly inspiring is the Gemmaverse.
Tens of thousands of model variants, tools, and libraries created by you, the developer community.
Gemma has been downloaded over 150 million times, and we've seen the community create close to 70,000 Gemma
variants, hundreds of which are specifically for the world's diverse languages, like the incredible story we shared last year of Navarasa,
a Gemma model fine-tuned to speak 15 Indic languages.
Gemma is available in over 140 languages and is the best multilingual open model on the planet.
And we're excited to announce that we're expanding Gemma even further to sign languages.
SignGemma-- that's a good one, guys. [APPLAUSE]
SignGemma is a new family of models trained to translate sign language to spoken language
texts. But it's best at American Sign Language in English.
[APPLAUSE] Thank you. It's the most capable sign language understanding model
ever. And we can't wait for you, developers and deaf and hard-of-hearing communities,
to take this foundation and build with it. And that's not all.
Gemma is also helping researchers expand our understanding to more than just human language.
Just last month, we introduced DolphinGemma, the world's first large language model for Dolphins.
AUDIENCE: [LAUGHS] GUS MARTINS: Yeah. [APPLAUSE]
Working with the researchers at Georgia Tech and the Wild Dolphin Project, DolphinGemma was fine-tuned on data
from decades of field research to help scientists better understand patterns
in how dolphins communicate. Can you imagine going to vacation someday and being
able to talk to a dolphin? [LAUGHTER] Let's check on the progress.
[VIDEO PLAYBACK] [FUN MUSIC] - When I was 12 years old, I would page through the encyclopedia.
I would stop on the whale and dolphin page, and I would go, I wonder what is going on in the minds of these animals.
Language, to me, is the ultimate question about intelligence, the thing that we
haven't been able to nail without a great tool. How do you decipher another species?
The main use of DolphinGemma is to eventually look at their natural language patterns and match it with the underwater video.
That's how we really figure out "their language," if they had it. All right. Beautiful day.
Woo-hoo. - Imagine a world where you could talk to an animal. DolphinGemma is the first LLM for Dolphins.
We've leveraged over 40 years of Denise's vocalization research to create this large language model to generate
new synthetic dolphin sounds. These sounds will help us to one day hopefully communicate with the dolphins.
[DOLPHIN CHITTERING SOUND EFFECT] - Thad shows up at my office one day, and he's wearing one of his many wearable computers.
And they're like, ooh, this guy's a techno man, right? - I was like, well, I do wearable computers. This sounds like a summer project.
We'll get this done for you. Here it is, 15 years later. We've engineered this CHAT JUNIOR
device to play our synthetic dolphin sounds underwater. Let's see which one this one is. [DOLPHIN WHISTLING]
That is the signature whistle for Denise. So whenever she's approaching the dolphin, she can introduce herself.
- So what they're doing is they're testing the distance at which the two chat boxes can efficiently recognize our synthetic sounds.
Oh, there we go. All right. We want it to work close to 100% before we take it in with the dolphins.
It's this dance between getting data and respecting their culture. - You know I'm not very seaworthy, so.
- Ah you did great. - The true breakthrough for me with DolphinGemma is that I can generate so many of these so quickly.
I just scroll through here and do days worth of work in minutes. If we're going to use this open model,
people would take advantage of Denise's 40 years worth of research and use it to model for their own purposes.
- Science is most exciting when you find something you're not expecting. You may discover something that you never even thought to look for.
And that's the beauty. You don't know what's coming. [MUSIC PLAYING]
[END PLAYBACK] [APPLAUSE]
Closing
JOSH WOODWARD: Isn't that amazing? The world's first large language model for dolphins.
And it was made to run on this right here, a Pixel 9 powered rig, just like this, so.
All right, I can't help myself. We're going to do one more demo, and then it's a wrap. Does that sound good? All right.
[APPLAUSE] So does everyone remembers the keynote companion app that Paige built right here at the desk?
Let's bring it back up on stage, and let's just see what it thought.
Hey, tell me, what did you think of today's announcements? KC: Oh, I thought they were amazing.
All the new features for developers are so exciting, especially the stuff with Gemini and the AI tools in Chrome.
It's going to make it so much-- JOSH WOODWARD: Wait. But what about my demo, the one I did?
KC: Your demo was fan-- JOSH WOODWARD: Oh. [LAUGHTER]
It's a live demo. What-- KC: Seriously, see? JOSH WOODWARD: What was some of the advice you'd
give people to build, if you don't want to talk about my demo? KC: I definitely recommend trying out the new AI
features in Chrome DevTools. It looks like it's going to make debugging so much easier, especially with the AI assistants
helping you fix issues directly. JOSH WOODWARD: All right, can you maybe just close us-- bid a fond farewell to the crowd.
What's on your mind? KC: Absolutely. Thank you all so much for being here today.
We've seen some incredible innovations, and I can't wait to see what amazing things you'll build with all the new tools.
Have a fantastic rest of I/O. JOSH WOODWARD: All right. [CHEERING, APPLAUSE]
Everything you've seen here is all going to be on I/O.Google
so you can check it out. And don't forget to join your local Google Developer program. You can connect to the local builders.
And I hope all of you enjoy the rest of I/O. We're excited to see how you're going to change the future of software development.
Thanks again. [APPLAUSE] [MUSIC PLAYING]